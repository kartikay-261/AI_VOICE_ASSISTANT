{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1840db-4f09-4748-af8b-c1725d9a7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import re\n",
    "import gradio as gr\n",
    "import os\n",
    "import torchvision\n",
    "import random\n",
    "import cProfile\n",
    "import subprocess\n",
    "import edge_tts\n",
    "import snakeviz\n",
    "import asyncio\n",
    "import gprof2dot\n",
    "import whisper\n",
    "import nest_asyncio\n",
    "import transformers\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "os.environ['FFMPEG_PATH'] = \"C:\\FFmpeg\\bin\\ffmpeg.exe\"\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78d5fca-7287-4fec-a569-c65bb6b8648c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3025ce0a7ca44843a5b2face9a9b969f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True, \n",
    "        device_map=\"auto\", \n",
    "        cache_dir=\"./model_docs\"\n",
    "       \n",
    ")\n",
    "model.eval() \n",
    "device_map = {\"module\": \"cuda:0\", \"module.encoder\": \"cuda:1\"}\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    truncation=True,\n",
    "    do_sample=True, \n",
    "    top_k=10, \n",
    "    num_return_sequences=1, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_workers=torch.cuda.device_count(),\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "\n",
    "template = PromptTemplate(input_variables=['input'], \n",
    "template=\"Generate a response for the following input: {input}\"\n",
    "       )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9272e42c-c5af-453c-b17b-1433e757e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAD(audio_file):\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    \n",
    "    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "    (get_speech_timestamps, _, read_audio, _, _) = utils\n",
    "    \n",
    "  \n",
    "    wav, original_sample_rate = librosa.load(audio_file, sr=None)\n",
    "    print(\"Original sample rate:\", original_sample_rate)\n",
    "    \n",
    "   \n",
    "    resampled_wav = librosa.resample(wav, orig_sr=original_sample_rate, target_sr=16000)\n",
    "    print(\"Resampled to 16 kHz\")\n",
    "    \n",
    " \n",
    "    speech_timestamps = get_speech_timestamps(resampled_wav, model, sampling_rate=16000)\n",
    "    print(\"Speech timestamps:\", speech_timestamps)\n",
    "    \n",
    "   \n",
    "    if not speech_timestamps:\n",
    "        raise ValueError(\"No speech detected in the audio file.\")\n",
    "    \n",
    " \n",
    "    trimmed_wav = []\n",
    "    for timestamp in speech_timestamps:\n",
    "        start_idx = timestamp['start']\n",
    "        end_idx = timestamp['end']\n",
    "        trimmed_wav.extend(resampled_wav[start_idx:end_idx])\n",
    "    \n",
    " \n",
    "    compiled_wav = np.array(trimmed_wav)\n",
    "    \n",
    "   \n",
    "    compiled_wav_resampled = librosa.resample(compiled_wav, orig_sr=16000, target_sr=16000)\n",
    "    \n",
    " \n",
    "    sf.write('compiled_resampled_audio.wav', compiled_wav_resampled, 16000)\n",
    "    print(\"Compiled and resampled audio saved as 'compiled_resampled_audio.wav'\")\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(wav)\n",
    "    plt.title('Original Audio')\n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(resampled_wav)\n",
    "    plt.title('Resampled Audio (16 kHz)')\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(compiled_wav_resampled)\n",
    "    plt.title('Compiled and Resampled Audio (16 kHz)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return compiled_wav_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e037e4-b665-4d57-aa06-dc9532fdbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def STT(audio_file):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_file)\n",
    "    \n",
    "    print(result[\"text\"])\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ee0e5d-700d-4ae4-9be4-52ba624e1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_Response(prompt):\n",
    "    prompt=str(prompt)\n",
    "    prompt=prompt.replace('.','')\n",
    "    promptl=prompt+\" Answer in two sentence or less\"\n",
    "   \n",
    "\n",
    "    response = chain.invoke(promptl)\n",
    "    \n",
    "    text_data = response['text']\n",
    "    print(text_data)\n",
    "    text_data = text_data.replace(promptl, '')\n",
    "    text_data = text_data.replace(prompt, '')\n",
    "    text_data = text_data.replace('Generate a response for the following input', '')\n",
    "    text_data = text_data.replace(':', '')\n",
    "    text_data = text_data.replace('-', '')\n",
    "    text_data = text_data.replace('Input','')\n",
    "    text_data = text_data.replace('Output','')\n",
    "    text_data = text_data.replace('INPUT','')\n",
    "    text_data = text_data.replace('OUTPUT','')\n",
    "    text_data = text_data.replace('Answer','')\n",
    "    text_data = text_data.replace('Response','')\n",
    "    text_data = text_data.replace('#', '')\n",
    "    text_data = text_data.replace('Generating an output of ','')\n",
    "    \n",
    "    print(text_data)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d5a37f-7880-4d13-968b-114c18653489",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def STT_Audio(TEXT, gender, rate, volume, pitch, max_retries=3):\n",
    "    voices = {\n",
    "        \"male\": [\"en-US-ChristopherNeural\", \"en-US-GuyNeural\", \"en-US-SteffanNeural\"],\n",
    "        \"female\": [\"en-US-EmmaNeural\", \"en-US-JennyNeural\", \"en-US-MichelleNeural\"]\n",
    "    }\n",
    "\n",
    "    if gender not in [\"male\", \"female\"]:\n",
    "        raise ValueError(\"Invalid gender. Please enter 'male' or 'female'.\")\n",
    "\n",
    "    voice = random.choice(voices[gender])\n",
    "    output_file = \"output_final.wav\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            communicate = edge_tts.Communicate(TEXT, voice, rate=rate, volume=volume, pitch=pitch)\n",
    "            await communicate.save(output_file)\n",
    "            print(f\"Audio saved to {output_file}\")\n",
    "            return output_file\n",
    "        except edge_tts.exceptions.NoAudioReceived as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "            await asyncio.sleep(1)  \n",
    "    raise RuntimeError(\"Failed to generate audio after multiple attempts\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc2acbe-76e8-4ca2-8102-9488d766942a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:6003\n",
      "Running on public URL: https://5fa9024e89d1d2bbcd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5fa9024e89d1d2bbcd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 406, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gradio\\route_utils.py\", line 760, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\middleware\\exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\routing.py\", line 754, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\routing.py\", line 774, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\routing.py\", line 295, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\responses.py\", line 348, in __call__\n",
      "    await send(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 510, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\h11\\_connection.py\", line 512, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\h11\\_connection.py\", line 545, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "Using cache found in C:\\Users\\ASUS/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample rate: 44100\n",
      "Resampled to 16 kHz\n",
      "Speech timestamps: [{'start': 28704, 'end': 56288}]\n",
      "Compiled and resampled audio saved as 'compiled_resampled_audio.wav'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_7348\\1596429053.py:57: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\whisper\\transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the capital of India?\n",
      "Generate a response for the following input:  What is the capital of India? Answer in two sentence or less. Response: The capital of India is New Delhi. It is a city located in the northern part of the country.\n",
      "\n",
      " .  The capital of India is New Delhi. It is a city located in the northern part of the country.\n",
      "\n",
      "Audio saved to output_final.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "async def pipeline_m(audio_file, gender, rate, volume, pitch):\n",
    "    CARW = VAD(audio_file)\n",
    "    Text = STT(CARW)\n",
    "    LLM_res = LLM_Response(Text)\n",
    "    processed_audio = await STT_Audio(LLM_res, gender, rate, volume, pitch)\n",
    "    return processed_audio\n",
    "\n",
    "\n",
    "async def process_audio(audio, gender, rate, volume, pitch):\n",
    "    return await pipeline_m(audio, gender, rate, volume, pitch)\n",
    "\n",
    "\n",
    "gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=[\n",
    "        gr.Audio(type=\"filepath\", label=\"Record Audio\"),  # Accepts audio input from microphone or file\n",
    "        gr.Dropdown(choices=[\"male\", \"female\"], label=\"gender\"),  # Gender selection\n",
    "        gr.Textbox(label=\"rate (+40%, -18%)\"),  # Rate adjustment as percentage\n",
    "        gr.Textbox(label=\"volume (+25%, -28%)\"),  # Volume adjustment as percentage\n",
    "        gr.Textbox(label=\"pitch (-39Hz)\")  # Pitch adjustment in Hz\n",
    "    ],\n",
    "    outputs=gr.Audio(type=\"filepath\"),  # Outputs the processed audio file and plays it\n",
    "    theme='freddyaboulton/dracula_revamped'  # Optional theme\n",
    ").launch(server_port=6003, share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d3439-3907-4537-bbe7-4f902ffdf9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15304fe8-d573-4ed2-b6ff-4d620df15050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
